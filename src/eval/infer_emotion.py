from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
import json
from tqdm import tqdm
import os
import argparse
import torch.distributed as dist
import warnings
import re

warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

from open_r1.prompts.emotion_prompt import INFER_PROMPT, GRPO_PROMPT

def set_seed(seed):
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

def setup_distributed():
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend="nccl")
    return local_rank, dist.get_world_size(), dist.get_rank()

def extract_aus(description):
    """ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–AUç¼–å·ï¼Œå¦‚AU6ã€AU12ç­‰ï¼Œè¿”å›å»é‡åˆ—è¡¨"""
    return sorted(set(re.findall(r"AU\d+", description)))

def extract_labels(description):
    """ä»<answer>æ ‡ç­¾ä¸­æå–labelï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”å¹¶å»é™¤ä¸¤ä¾§ç©ºç™½"""
    answer_match = re.search(r"<answer>(.*?)</answer>", description, flags=re.DOTALL)
    if not answer_match:
        return []
    labels_text = answer_match.group(1)
    # ç”¨ , ã€ ï¼Œåˆ†å‰²ï¼Œå»é™¤å‰åç©ºç™½
    return sorted(set(label.strip() for label in re.split(r"[,\u3001ï¼Œ]", labels_text) if label.strip()))

def parse_args():
    parser = argparse.ArgumentParser(description="Qwen2.5-VL inference: only replace 'description' field")
    parser.add_argument("--main_rank", type=int, default=0)
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    parser.add_argument("--image_root", type=str, required=True)
    parser.add_argument("--config_json", type=str, required=True)
    parser.add_argument("--prompt_mode", type=str, default="sft", choices=["sft", "grpo"])
    parser.add_argument("--bsz", type=int, default=4)
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--max_retry", type=int, default=5, help="Maximum retry times for incorrect predictions")
    return parser.parse_args()


def get_error_messages(gold_aus, gold_labels, pred_aus, pred_labels, output):
    """è·å–é”™è¯¯ä¿¡æ¯åˆ—è¡¨"""
    errors = []
    
    # æ£€æŸ¥AUé”™è¯¯
    if gold_aus:  # åªåœ¨æœ‰gold_ausæ—¶æ£€æŸ¥
        if gold_aus != pred_aus:
            errors.append(f"The predicted AUs must be: {gold_aus}")
    
    # æ£€æŸ¥labelé”™è¯¯
    if gold_labels:  # åªåœ¨æœ‰gold_labelsæ—¶æ£€æŸ¥
        if gold_labels != pred_labels:
            errors.append(f"The predicted labels must be: {gold_labels}")

    # æ£€æŸ¥è¾“å‡ºæ ¼å¼
    think_match = re.search(r"<think>(.*?)</think>", output, flags=re.DOTALL)
    if not think_match or not think_match.group(1).strip():
        errors.append("Missing or empty <think>...</think> tag.")
    answer_match = re.search(r"<answer>(.*?)</answer>", output, flags=re.DOTALL)
    if not answer_match or not answer_match.group(1).strip():
        errors.append("Missing or empty <answer>...</answer> tag.")

    return errors


def print_retry_info(item_id, image, retry_num, pred_aus, pred_labels, match_aus, match_labels, errors, output, rank):
    """æ‰“å°å•æ¬¡é‡è¯•ä¿¡æ¯"""
    if rank == 0:  # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        print(f"\n===== æ ·æœ¬ID: {item_id}, å›¾ç‰‡: {image}, å°è¯• #{retry_num} =====")
        print(f"é¢„æµ‹AUs: {pred_aus}")
        print(f"é¢„æµ‹labels: {pred_labels}")
        print(f"AUåŒ¹é…: {match_aus}, labelåŒ¹é…: {match_labels}")
        if errors:
            print(f"é”™è¯¯: {', '.join(errors)}")
        print(f"è¾“å‡º: {output}")
        print("="*60)

def main():
    set_seed(42)
    try:
        local_rank, world_size, rank = setup_distributed()
    except:
        # å•å¡å•è¿›ç¨‹ç¯å¢ƒ
        local_rank, world_size, rank = 0, 1, 0
    device = f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu"
    args = parse_args()

    with open(args.config_json, 'r', encoding='utf-8') as f:
        config = json.load(f)

    # inferenceæ•°æ®é›†çš„emotionsç¡®å®šï¼ˆéœ€æ»¡è¶³æ¥å£é€šç”¨æ€§ï¼Œå¯æ ¹æ®ä½ çš„è¯„ä¼°æ•°æ®keyè‡ªè¡Œå¾®è°ƒï¼‰
    input_basename = os.path.basename(args.input_file)
    emotions = config.get(input_basename, {}).get("emotions", [])

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.model_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        device_map={"": local_rank} if torch.cuda.is_available() else None,
    )
    processor = AutoProcessor.from_pretrained(args.model_path)

    # è¯»å–è¾“å…¥jsonl
    data = []
    with open(args.input_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))

    per_rank = len(data) // world_size
    start_idx = rank * per_rank
    end_idx = start_idx + per_rank if rank < world_size - 1 else len(data)
    rank_data = data[start_idx:end_idx]

    # å®šä¹‰é‡è¯•æç¤ºè¯æ¨¡æ¿
    retry_template = """
### Previous Response Issues
Here is your previous response:
{prev_response}

Your previous response had the following issues:
{issues}

Please correct these issues in your new response. Make sure to:
1. Include all required Action Units in your analysis
2. Provide the correct emotion in your answer
3. Use proper <think>...</think> and <answer>...</answer> tags
4. Avoid negative expressions like "no", "not", "without"
5. Be concise and precise
""".strip()

    # åˆå§‹æ¨ç†å’Œé‡è¯•
    rank_outputs = []
    for idx, item in enumerate(tqdm(rank_data, desc=f"Rank {rank} inference", disable=rank!=0)):
        item_id = item.get("id", f"item_{idx}")
        img_name = item.get("image", "")
        
        img_path = os.path.join(args.image_root, img_name) if 'image' in item else None
        question = item['question'].replace('<image>','').strip() if 'question' in item and item['question'] else "What is the emotion of this face?"
        
        if not img_path:
            rank_outputs.append(None)
            continue
            
        # è·å–çœŸå®æ ‡ç­¾
        gold_aus = sorted(set(item.get("AUs", [])))
        gold_labels = sorted(set(item.get("labels", [])))
        
        # ä½¿ç”¨GRPO_PROMPTä½œä¸ºåŸºç¡€æç¤ºè¯
        curr_prompt = GRPO_PROMPT.format(
            Question=question, 
            Emotions=emotions.keys(),
        )
        
        # æ ¹æ®æ˜¯å¦æœ‰AUså’ŒlabelsåŠ¨æ€æ·»åŠ Ground Truthéƒ¨åˆ†
        gt_parts = []
        if gold_aus:
            gt_parts.append(f"Your analysis MUST identify these specific Action Units: {gold_aus}")
        if gold_labels:
            gt_parts.append(f"And your final answer MUST be this exact emotion: {gold_labels}")
            
        # å¦‚æœæœ‰Ground Truthä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºè¯
        if gt_parts:
            gt_section = "### Ground Truth\n" + "\n".join(gt_parts)
            curr_prompt += "\n\n" + gt_section
        
        # æœ€å¤šé‡è¯•æ¬¡æ•°ï¼ˆåŒ…æ‹¬åˆå§‹å°è¯•ï¼‰
        best_output = None
        is_correct = False
        
        for retry in range(args.max_retry + 1):  # +1 æ˜¯å› ä¸ºåŒ…æ‹¬åˆå§‹å°è¯•
            # å‡†å¤‡å•æ¡æ•°æ®çš„æ¶ˆæ¯
            message = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": f"file://{img_path}"},
                        {"type": "text", "text": curr_prompt}
                    ]
                }
            ]
            
            # å•æ¡æ•°æ®çš„å¤„ç†
            text = processor.apply_chat_template(message, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info([message])
            inputs = processor(
                text=text,
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            ).to(device)
            
            gen = model.generate(**inputs, use_cache=True, max_new_tokens=args.max_new_tokens, do_sample=False)
            trimmed = gen[0][len(inputs.input_ids[0]):]
            output = processor.decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)
            
            # ç¬¬ä¸€æ¬¡å°è¯•çš„ç»“æœä¿å­˜ä¸ºbest_outputï¼Œä»¥é˜²åç»­éƒ½å¤±è´¥
            if retry == 0:
                best_output = output
            
            # éªŒè¯ç»“æœ
            pred_aus = extract_aus(output)
            pred_labels = extract_labels(output)
            
            # éªŒè¯é€»è¾‘ - åªéªŒè¯æœ‰çœŸå®å€¼çš„éƒ¨åˆ†
            match_aus = True if not gold_aus else gold_aus == pred_aus
            match_labels = True if not gold_labels else gold_labels == pred_labels
            # if match_labels == False:
            #     print('pred_labels', pred_labels)
            #     print('gold_labels', gold_labels)
            #     print(output)
            #     exit()
            
            # è·å–é”™è¯¯ä¿¡æ¯
            errors = get_error_messages(gold_aus, gold_labels, pred_aus, pred_labels, output)
            
            # å³æ—¶æ‰“å°é‡è¯•ä¿¡æ¯ï¼ˆå¦‚æœæœ‰é”™è¯¯ï¼‰
            if retry > 0 or not (match_aus and match_labels):
                print_retry_info(item_id, img_name, retry, pred_aus, pred_labels, 
                                match_aus, match_labels, errors, output, rank)
            
            # å¦‚æœåŒ¹é…æˆåŠŸï¼Œä¿å­˜è¯¥ç»“æœ
            if match_aus and match_labels:
                best_output = output
                is_correct = True
                if retry > 0:  # å¦‚æœæ˜¯é‡è¯•æˆåŠŸï¼Œæ‰“å°æˆåŠŸä¿¡æ¯
                    if rank == 0:
                        print(f"\nâœ… æ ·æœ¬ {item_id} åœ¨ç¬¬ {retry} æ¬¡é‡è¯•åæˆåŠŸä¿®æ­£ï¼")
                break
                
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™å‡†å¤‡é‡è¯•
            if retry < args.max_retry and errors:
                # æ„å»ºé‡è¯•æç¤ºè¯
                curr_prompt = curr_prompt + '\n\n' + retry_template.format(
                    prev_response=output,
                    issues=", ".join(errors)
                )
                
                if rank == 0:
                    print(f"ğŸ”„ æ ·æœ¬ {item_id} å‡†å¤‡ç¬¬ {retry+1} æ¬¡é‡è¯•...")
                
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ‰“å°æœ€ç»ˆå¤±è´¥ä¿¡æ¯
        if not is_correct and rank == 0:
            print(f"\nâŒ æ ·æœ¬ {item_id} ç»è¿‡ {args.max_retry} æ¬¡é‡è¯•åä»æœªæˆåŠŸã€‚")
            
        # ä¿å­˜æœ€å¥½çš„ç»“æœï¼ˆå¯èƒ½æ˜¯æ­£ç¡®çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯æ‰€æœ‰å°è¯•éƒ½å¤±è´¥åçš„åˆå§‹ç»“æœï¼‰
        rank_outputs.append((best_output, is_correct))
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_outputs = [(None, False)] * len(data)
    rank_res = [(start_idx + i, out) for i, out in enumerate(rank_outputs)]
    try:
        gathered = [None] * world_size
        dist.all_gather_object(gathered, rank_res)
    except:
        gathered = [rank_res]
        
    if rank == args.main_rank:
        for part in gathered:
            for idx, out in part:
                all_outputs[idx] = out

        # ç»Ÿè®¡å’Œå†™å…¥ç»“æœ
        total = 0
        correct = 0
        filtered_data = []
        for i, item in enumerate(data):
            if all_outputs[i] is not None:  # ç¡®ä¿æœ‰è¾“å‡º
                total += 1
                output, is_correct = all_outputs[i]
                
                if is_correct:
                    correct += 1
                    output_item = item.copy()
                    output_item["description"] = output
                    filtered_data.append(output_item)
        
        # å†™å…¥ä»…åŒ…å«æ­£ç¡®é¢„æµ‹çš„è¾“å‡ºjsonl
        os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
        with open(args.output_file, "w", encoding="utf-8") as f:
            for item in filtered_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n===== æœ€ç»ˆç»Ÿè®¡ =====")
        print(f"æ€»æ ·æœ¬æ•°: {total}")
        print(f"AUså’Œlabelså‡æ­£ç¡®çš„æ ·æœ¬æ•°: {correct}")
        if total > 0:
            print(f"å‡†ç¡®ç‡: {correct/total:.4f}")
        else:
            print("æ— æœ‰æ•ˆæ ·æœ¬")
            
        print(f"Results saved to {args.output_file}")

    try:
        dist.barrier()
    except:
        pass

if __name__ == "__main__":
    main()